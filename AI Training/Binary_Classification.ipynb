{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Selected features for training\n",
    "selected_features = [\n",
    "    'Flow Duration', 'Flow Bytes/s', 'Flow Packets/s',\n",
    "    'Total Fwd Packets', 'Total Backward Packets', 'Average Packet Size', 'Packet Length Std',\n",
    "    'Flow IAT Mean', 'Flow IAT Std', 'Fwd IAT Mean', 'Bwd IAT Mean',\n",
    "    'SYN Flag Count', 'ACK Flag Count', 'RST Flag Count'\n",
    "]\n",
    "\n",
    "# File paths for the Gradient Boosting training datasets\n",
    "gradient_boosting_train_files = [\n",
    "    'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv',\n",
    "    'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
    "    'Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
    "    'Monday-WorkingHours.pcap_ISCX.csv',\n",
    "    'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
    "    'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
    "    'Tuesday-WorkingHours.pcap_ISCX.csv',\n",
    "    'Wednesday-workingHours.pcap_ISCX.csv',\n",
    "    'Friday-02-03-2018_TrafficForML_CICFlowMeter.csv',\n",
    "    'Friday-16-02-2018_TrafficForML_CICFlowMeter.csv',\n",
    "    'Friday-23-02-2018_TrafficForML_CICFlowMeter.csv',\n",
    "    'Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv',\n",
    "    'Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv',\n",
    "    'Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv',\n",
    "    'Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv',\n",
    "    'Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv',\n",
    "    'Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv',\n",
    "    'Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv'\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Load and Preprocess Data for Gradient Boosting\n",
    "# -----------------------------\n",
    "print(\"Loading and concatenating Gradient Boosting training datasets...\")\n",
    "gb_data = pd.concat([pd.read_csv(file) for file in gradient_boosting_train_files])\n",
    "\n",
    "# Renaming columns by removing leading/trailing whitespace\n",
    "gb_data.columns = gb_data.columns.str.strip()\n",
    "\n",
    "# Dropping duplicates and handling NaN and infinite values\n",
    "gb_data = gb_data.drop_duplicates()\n",
    "gb_data = gb_data.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Fill missing values for specific columns in testing data\n",
    "for col in selected_features:\n",
    "    if col in gb_data.columns:\n",
    "        gb_data[col] = gb_data[col].fillna(gb_data[col].median())\n",
    "\n",
    "# Encode labels (0 = BENIGN, 1 = MALICIOUS)\n",
    "gb_data['Label'] = gb_data['Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)\n",
    "\n",
    "# Prepare features and labels for Gradient Boosting\n",
    "X_gb_train = gb_data[selected_features]\n",
    "y_gb_train = gb_data['Label']\n",
    "\n",
    "# -----------------------------\n",
    "# Combine Data for Scaling and Standardize\n",
    "# -----------------------------\n",
    "# Fit a single scaler on combined benign data\n",
    "print(\"Fitting scaler and scaling data...\")\n",
    "scaler = StandardScaler()\n",
    "X_gb_train_scaled = scaler.transform(X_gb_train)\n",
    "\n",
    "# Save the scaler and selected features\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "joblib.dump(selected_features, 'selected_features.joblib')\n",
    "print(\"Scaler and selected features have been saved.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Train Gradient Boosting\n",
    "# -----------------------------\n",
    "print(\"Splitting data into training and testing sets for Gradient Boosting...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_gb_train_scaled, y_gb_train, test_size=0.2, random_state=42, stratify=y_gb_train\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Gradient Boosting...\")\n",
    "gb = GradientBoostingClassifier(n_estimators=200, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Save the Gradient Boosting model\n",
    "joblib.dump(gb, 'gradient_boosting_model.joblib')\n",
    "print(\"Gradient Boosting model has been saved.\")\n",
    "\n",
    "# Evaluate Gradient Boosting\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "print(\"\\nGradient Boosting Classification Report:\\n\", classification_report(y_test, y_pred_gb))\n",
    "print(\"Gradient Boosting Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_gb))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
