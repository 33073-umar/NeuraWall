{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Selected features for training\n",
    "selected_features = [\n",
    "    'Flow Duration', 'Flow Bytes/s', 'Flow Packets/s',\n",
    "    'Total Fwd Packets', 'Total Backward Packets', 'Average Packet Size', 'Packet Length Std',\n",
    "    'Flow IAT Mean', 'Flow IAT Std', 'Fwd IAT Mean', 'Bwd IAT Mean',\n",
    "    'SYN Flag Count', 'ACK Flag Count', 'RST Flag Count'\n",
    "]\n",
    "\n",
    "# File paths for the One-Class SVM baseline datasets\n",
    "oneclass_svm_train_files = [\n",
    "    'Dataset/normal_traffic_labeled.csv',\n",
    "    'Dataset//home_traffic_labeled.csv'\n",
    "]\n",
    "\n",
    "# File paths for the Gradient Boosting training datasets\n",
    "gradient_boosting_train_files = [\n",
    "    'Dataset/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv',\n",
    "    'Dataset/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
    "    'Dataset/Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
    "    'Dataset/Monday-WorkingHours.pcap_ISCX.csv',\n",
    "    'Dataset/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
    "    'Dataset/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
    "    'Dataset/Tuesday-WorkingHours.pcap_ISCX.csv',\n",
    "    'Dataset/Wednesday-workingHours.pcap_ISCX.csv'\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Load and Preprocess Data for One-Class SVM\n",
    "# -----------------------------\n",
    "print(\"Loading and concatenating One-Class SVM baseline datasets...\")\n",
    "svm_data = pd.concat([pd.read_csv(file) for file in oneclass_svm_train_files])\n",
    "\n",
    "# Renaming columns by removing leading/trailing whitespace\n",
    "svm_data.columns = svm_data.columns.str.strip()\n",
    "\n",
    "# Dropping duplicates and handling NaN and infinite values\n",
    "svm_data = svm_data.drop_duplicates()\n",
    "svm_data = svm_data.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Fill missing values for specific columns in testing data\n",
    "for col in selected_features:\n",
    "    if col in svm_data.columns:\n",
    "        svm_data[col] = svm_data[col].fillna(svm_data[col].median())\n",
    "\n",
    "# Encode labels (0 = BENIGN, 1 = MALICIOUS)\n",
    "svm_data['Label'] = svm_data['Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)\n",
    "\n",
    "# Prepare features for One-Class SVM, using only BENIGN samples\n",
    "X_svm_train = svm_data.loc[svm_data['Label'] == 0, selected_features].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# Load and Preprocess Data for Gradient Boosting\n",
    "# -----------------------------\n",
    "print(\"Loading and concatenating Gradient Boosting training datasets...\")\n",
    "gb_data = pd.concat([pd.read_csv(file) for file in gradient_boosting_train_files])\n",
    "\n",
    "# Renaming columns by removing leading/trailing whitespace\n",
    "gb_data.columns = gb_data.columns.str.strip()\n",
    "\n",
    "# Dropping duplicates and handling NaN and infinite values\n",
    "gb_data = gb_data.drop_duplicates()\n",
    "gb_data = gb_data.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Fill missing values for specific columns in testing data\n",
    "for col in selected_features:\n",
    "    if col in gb_data.columns:\n",
    "        gb_data[col] = gb_data[col].fillna(gb_data[col].median())\n",
    "\n",
    "# Encode labels (0 = BENIGN, 1 = MALICIOUS)\n",
    "gb_data['Label'] = gb_data['Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)\n",
    "\n",
    "# Prepare features and labels for Gradient Boosting\n",
    "X_gb_train = gb_data[selected_features]\n",
    "y_gb_train = gb_data['Label']\n",
    "\n",
    "# -----------------------------\n",
    "# Combine Data for Scaling and Standardize\n",
    "# -----------------------------\n",
    "# Fit a single scaler on combined benign data\n",
    "print(\"Fitting scaler and scaling data...\")\n",
    "scaler = StandardScaler()\n",
    "X_combined_benign = pd.concat([X_svm_train, X_gb_train[y_gb_train == 0]])\n",
    "scaler.fit(X_combined_benign)\n",
    "\n",
    "# Scale training data for both models\n",
    "X_svm_train_scaled = scaler.transform(X_svm_train)\n",
    "X_gb_train_scaled = scaler.transform(X_gb_train)\n",
    "\n",
    "# Save the scaler and selected features\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "joblib.dump(selected_features, 'selected_features.joblib')\n",
    "print(\"Scaler and selected features have been saved.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Train One-Class SVM\n",
    "# -----------------------------\n",
    "print(\"Training One-Class SVM on BENIGN samples from One-Class SVM training data...\")\n",
    "svm_model = OneClassSVM(kernel=\"rbf\",  gamma=0.08, nu=0.05)\n",
    "svm_model.fit(X_svm_train_scaled)\n",
    "\n",
    "# Save the One-Class SVM model\n",
    "joblib.dump(svm_model, 'oneclass_svm_model.joblib')\n",
    "print(\"One-Class SVM model has been saved.\")\n",
    "\n",
    "# Evaluate One-Class SVM on its own baseline data\n",
    "X_svm_test_scaled = scaler.transform(svm_data[selected_features])\n",
    "y_svm_test = svm_data['Label']\n",
    "y_svm_pred = np.where(svm_model.predict(X_svm_test_scaled) == -1, 1, 0)\n",
    "print(\"Classification Report (One-Class SVM):\\n\", classification_report(y_svm_test, y_svm_pred))\n",
    "print(\"Confusion Matrix (One-Class SVM):\\n\", confusion_matrix(y_svm_test, y_svm_pred))\n",
    "\n",
    "# -----------------------------\n",
    "# Train Gradient Boosting\n",
    "# -----------------------------\n",
    "print(\"Splitting data into training and testing sets for Gradient Boosting...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_gb_train_scaled, y_gb_train, test_size=0.2, random_state=42, stratify=y_gb_train\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Gradient Boosting...\")\n",
    "gb = GradientBoostingClassifier(n_estimators=200, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Save the Gradient Boosting model\n",
    "joblib.dump(gb, 'gradient_boosting_model.joblib')\n",
    "print(\"Gradient Boosting model has been saved.\")\n",
    "\n",
    "# Evaluate Gradient Boosting\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "print(\"\\nGradient Boosting Classification Report:\\n\", classification_report(y_test, y_pred_gb))\n",
    "print(\"Gradient Boosting Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_gb))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
